{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidade Federal de Alagoas\n",
    "\n",
    "Instituto de Computação\n",
    "\n",
    "Orientanda: Júlia Albuquerque Aguiar\n",
    "\n",
    "Orientador: André Lage\n",
    "\n",
    "[Repositório no Github](https://github.com/juliaguiar/TCC/)\n",
    "\n",
    "# Código-fonte para protótipo de TCC\n",
    "\n",
    "## Arquivo `violence.py`\n",
    "\n",
    "A classe `Violence` tem dois atributos: um `type` que representa o tipo de violência - sexual, física, psicológica, moral ou patrimonial - e um `value` que representa o peso desse tipo; quanto maior o peso, mais provável que esse tipo represente o relato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Violence(object):\n",
    "  def __init__(self, value, type):\n",
    "    self.__value = value\n",
    "    self.__type = type\n",
    "\n",
    "  def __repr__(self):\n",
    "    return \"value:%s type:%s\" % (self.__value, self.__type)\n",
    "\n",
    "  def get_value(self):\n",
    "    return self.__value\n",
    "\n",
    "  def get_type(self):\n",
    "    return self.__type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquivo `myTweetTokenizer.py`\n",
    "\n",
    "Nesse arquivo é feita a personalização do tokenizer considerando as especificidades de um texto de tweet, cujo conteúdo tem muitos caracteres distintos do que são tratados por *default* no **SpaCy**: [How spaCy’s tokenizer works](https://spacy.io/usage/linguistic-features#how-tokenizer-works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import twikenizer as twk\n",
    "\n",
    "twk = twk.Twikenizer()\n",
    "\n",
    "class MyTweetTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, tweet):\n",
    "        words = twk.tokenize(tweet)\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Repositório](https://github.com/Guilherme-Routar/Twikenizer) do autor da biblioteca e seu [artigo](https://www.voxpol.eu/download/ma_thesis/326963_2.pdf) sobre a pesquisa em que é analisada as diferenças entre o resultado desse trabalho comparado ao tokenizer padrão do **SpaCy** e **NLTK** em *Table 3.3* da seção \"3.2 Tweets tokenization\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquivo `classificationViolence.py`\n",
    "\n",
    "### Imports e Variáveis\n",
    "\n",
    "Começamos com os imports: `spacy` é a biblioteca de NLP de Python, `tweepy` é a biblioteca para acessar a API do Twitter e `pandas` é uma biblioteca de análise e estrutura de dados com Python que usaremos para criar um *DataFrame*; os outros são para acessar as outras classes do projeto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from dev.myTweetTokenizer import MyTweetTokenizer\n",
    "from dev.violence import Violence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando `pandas` criamos um _DataFrame_ e colocamos os dados com sua **Data** (`column=\"data\"`) e **Texto** (`column=\"post\"`) que já vêm com a captura das postagens pela API e o **Tipo de Violência** (`column=\"tipoDeViolencia\"`) que é gerado pelo algoritmo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"data\", \"post\", \"tipoDeViolencia\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos **dicionários** para cada um dos 5 **tipos de violência**. Esses verbos devem ser representativos sobre qual tipo de violência se trata, ou seja, os verbos mais frequentes nos relatos de cada tipo. Os que estão escritos a seguir foram adicionados apenas para simulação e deverão ser a entrada do sistema para que o usuário final possa adicioná-los:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO citar no TCC q vc dividiu nesses tipos de violência de acordo com os tipos categorizados no Art. 7 da Lei MP\n",
    "dictionaryKeywordsSexualViolence = [\"estuprar\", \"abusar\", \"assediar\", \"agarrar\"]\n",
    "dictionaryKeywordsPhysicalViolence = [\"bater\", \"machucar\", \"espancar\", \"empurrar\"]\n",
    "dictionaryKeywordsPsychologicalViolence = [\"ameaçar\", \"humilhar\", \"xingar\", \"ofender\"]\n",
    "dictionaryKeywordsMoralViolence = [\"acusar\", \"chantagear\", \"ofender\", \"caluniar\"]\n",
    "dictionaryKeywordsPatrimonialViolence = [\"quebrar\", \"destruir\", \"pegar\", \"roubar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que foi feita a listagem de palavras, também foram listados **pesos** para cada uma dessas palavras baseado na sua frequência nos relatos. Os que estão escritos a seguir foram adicionados apenas para simulação e também deverão ser a entrada do sistema para que o usuário final possa adicioná-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsForKeywordsSexualViolence = [4, 3, 2, 1]\n",
    "weightsForKeywordsPhysicalViolence = [3, 2, 4, 1]\n",
    "weightsForKeywordsPsychologicalViolence = [4, 2, 1, 3]\n",
    "weightsForKeywordsMoralViolence = [2, 4, 1, 3]\n",
    "weightsForKeywordsPatrimonialViolence = [2, 4, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir estão listadas as **variáveis de autenticação** para acesso da *API do Twitter*, que são individuais e geradas automaticamente quando criado o projeto no *Dashboard* da sua conta de desenvolvedor da rede social:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'put your key here'\n",
    "consumer_secret = 'put your key here'\n",
    "token_key = 'put your key here'\n",
    "token_secret = 'put your key here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções\n",
    "\n",
    "A `accessApiTwitter` faz a autenticação utilizando as variáveis acima mostradas e retorna um objeto de **wrapper** para API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accessApiTwitter():\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(token_key, token_secret)\n",
    "\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `searchHashtagReportOnTwitter` recebe a `hashtagName` que será pesquisada, como exemplos\n",
    "\n",
    "- #EuViviUmRelacionamentoAbusivo\n",
    "- #maselenuncamebateu\n",
    "- #MeuExAbusivo\n",
    "- #érelacionamentoabusivoquando\n",
    "- #elenãotebate\n",
    "\n",
    "um `dateSince` que é a data de início para a pesquisa - que pegará todas as postagens feitas com `hashtagName` de `dateSince` até agora - e `accessPackageApiTwitter` que é o objeto de **wrapper** retornado na função anteior `accessApiTwitter`. A função vai preenchendo dentro do *loop* o **dataframe** `df` com cada postagem pegando sua data de criação e texto:   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchHashtagReportOnTwitter(hashtagName, dateSince, accessPackageApiTwitter):\n",
    "    global df\n",
    "\n",
    "    for tweet in tweepy.Cursor(accessPackageApiTwitter.search,\n",
    "                               q=hashtagName,\n",
    "                               lang=\"pt\",\n",
    "                               since=dateSince,\n",
    "                               tweet_mode=\"extended\").items(10):\n",
    "        df = df.append({\"data\": tweet.created_at, \"post\": tweet.full_text}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `reportTokenization` faz a tokenização (já personalizada) do tweet e retorna um `doc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportTokenization(tweet):\n",
    "    doc = nlp(tweet)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hooking an arbitrary tokenizer into the pipeline](https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg \"SpaCy Pipeline\")\n",
    "\n",
    "Em `selectOnlyVerbsReport` é colocado em uma `list` apenas os verbos de cada texto da postagem. O SpaCy tem um recurso `pos_` do objeto `Token` que diz qual seu tipo, se é **VERB**, **NOUN** ou outras combinações como pode ser visto em [Linguistic Features - POS Tagging](https://spacy.io/usage/linguistic-features#pos-tagging).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectOnlyVerbsReport(doc):\n",
    "    list = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            list.append(token)\n",
    "\n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns opções de visualizações:\n",
    "\n",
    "![Visualizing the dependency parse](https://spacy.io/displacy-3504502e1d5463ede765f0a789717424.svg \"Visualizing the dependency parse\")\n",
    "\n",
    "![Visualizing the dependency parse](https://spacy.io/displacy-compact-4c063e533e7ca1019a2f763ed5b7a925.svg \"Visualizing the dependency parse\")\n",
    "\n",
    "\n",
    "\n",
    "O `sumOfList` serve apenas para fazer a soma dos valores em uma lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumOfList(list):\n",
    "    sum = 0\n",
    "    for num in list:\n",
    "        sum += num\n",
    "\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em `createNumericRepresentation` o objetivo é retornar uma **representação numérica** do texto para cada tipo de violência. Ou seja, atribuir uma representação de similaridade para cada verbo no dicionário em comparação aos verbos do texto em análise. Ter essas representações irá nos ajudar a entender com qual tipo de violência o relato mais se assemelha. Observe que os tamanhos das `listNumericRepresentation` retornadas são sempre iguais ao tamanho do dicionário, porque é um tamanho que pode ser fixado e se tornar padrão já que a quantidade de verbos por relatos não têm um tamanho fixo. Ao final do cálculo da média de similaridade, é feita uma multiplicação com o peso de cada palavra do dicionário que também são a entrada no nosso sistema como já foi explicado: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNumericRepresentation(list, dictionary, weight):\n",
    "    listNumericRepresentation = []\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "    for index, word in enumerate(dictionary):\n",
    "        doc = nlp(word)\n",
    "        sum = 0\n",
    "        for token in list:\n",
    "            sum += doc.similarity(token)\n",
    "        mean = sum / len(list)\n",
    "\n",
    "        listNumericRepresentation.append(mean*weight[index])\n",
    "\n",
    "    return listNumericRepresentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando retornado as listas, teremos para cada dicionário uma representação numérica do texto em cima desses dicionários, como exemplo:\n",
    "\n",
    "- [\"estuprar\", \"abusar\", \"assediar\", \"agarrar\"] => [0.8, 0.5, 0.3, 0.1]\n",
    "- [\"bater\", \"machucar\", \"espancar\", \"empurrar\"] => [0.7, 0.5, 0.2, 0.1]\n",
    "- [\"ameaçar\", \"humilhar\", \"xingar\", \"ofender\"] => [0.3, 0.5, 0.9, 0.2]\n",
    "- [\"acusar\", \"chantagear\", \"ofender\", \"caluniar\"] => [0.4, 0.6, 0.0, 0.9]\n",
    "- [\"quebrar\", \"destruir\", \"pegar\", \"roubar\"] => [0.8, 0.7, 0.3, 0.2]\n",
    "\n",
    "e na função `typeAnalysis` preenchemos uma *list* `listViolence` de objetos `Violence` registrados no arquivo `violence.py` explicado no primeiro item desse documento. Depois é feito um `sorted` com o `value` de cada `type` e rankeado os tipos do mais provável para o menos provável retornando em `typeViolenceSorted`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeAnalysis(list):\n",
    "\n",
    "    listViolence = []\n",
    "\n",
    "    listNumRepresSexualViolence = createNumericRepresentation(list, dictionaryKeywordsSexualViolence, weightsForKeywordsSexualViolence)\n",
    "    listViolence.append(Violence(-sumOfList(listNumRepresSexualViolence), 'Violência Sexual'))\n",
    "    \n",
    "    listNumRepresPhysicalViolence = createNumericRepresentation(list, dictionaryKeywordsPhysicalViolence, weightsForKeywordsPhysicalViolence)\n",
    "    listViolence.append(Violence(-sumOfList(listNumRepresPhysicalViolence), 'Violência Física'))\n",
    "\n",
    "    listNumRepresPsychologicalViolence = createNumericRepresentation(list, dictionaryKeywordsPsychologicalViolence, weightsForKeywordsPsychologicalViolence)\n",
    "    listViolence.append(Violence(-sumOfList(listNumRepresPsychologicalViolence), 'Violência Psicológica'))\n",
    "\n",
    "    listNumRepresMoralViolence = createNumericRepresentation(list, dictionaryKeywordsMoralViolence, weightsForKeywordsMoralViolence)\n",
    "    listViolence.append(Violence(-sumOfList(listNumRepresMoralViolence), 'Violência Moral'))\n",
    "\n",
    "    listNumRepresPatrimonialViolence = createNumericRepresentation(list, dictionaryKeywordsPatrimonialViolence, weightsForKeywordsPatrimonialViolence)\n",
    "    listViolence.append(Violence(-sumOfList(listNumRepresPatrimonialViolence), 'Violência Patrimonial'))\n",
    "\n",
    "    typeViolenceSorted = sorted(listViolence, key=Violence.get_value)\n",
    "\n",
    "    return typeViolenceSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal\n",
    "\n",
    "Aqui é utilizada a classe do arquivo `myTweetTokenizer.py` - mostrado no segundo item desse documento - para retornar e inserir um tokenizer específico na estrutura padrão do SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "nlp.tokenizer = MyTweetTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segue a iteração por todas as linhas do *DataFrame* selecionando `row[1]` onde está o texto da postagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    doc = reportTokenization(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É selecionado apenas o `type` do retorno da função `typeAnalysis` e adicionado no *DataFrame* `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeViolenceSorted = [v.get_type() for v in valueAndTypeViolenceSorted]\n",
    "    df.at[index, 'tipoDeViolencia'] = typeViolenceSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E finalmente o *DataFrame* é registrado no `csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datasetReports.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
